{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb19ce-a28d-44cc-b982-417d3d0366c5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.2/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /research/cbim/vast/zl502/anaconda3/envs/peft_agi did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /research/cbim/vast/zl502/anaconda3/envs/peft_agi/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftModelForCausalLM, prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoFeatureExtractor\n",
    "\n",
    "from general_dataset import GeneralDataset\n",
    "from agi_utils import *\n",
    "from tqdm import tqdm\n",
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "import random\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from combine_model_seq import SeqCombine\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a1e17d-2c29-4671-838a-d2f5185b1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [00:06<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "assign openagi data path \n",
    "\"\"\"\n",
    "data_path = \"YOUR_DATA_PATH\"\n",
    "\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "test_dataloaders = []\n",
    "for i in tqdm(test_task_idx):\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=20)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f039c632-0136-4490-87ab-3fa1564e07ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57b5e53196248d29dd332d347538e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 13022417920 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "# base_model = \"eachadea/vicuna-7b-1.1\"\n",
    "base_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# base_model = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# base_model = \"chainyo/alpaca-lora-7b\"\n",
    "load_8bit = True\n",
    "\n",
    "hf_token = \"YOUR_HUGGINGFACE_KEY\"\n",
    "\n",
    "max_memory_mapping = {\n",
    "    0: \"48GB\",\n",
    "    1: \"48GB\",\n",
    "    2: \"48GB\",\n",
    "    3: \"48GB\",\n",
    "    4: \"0GB\",\n",
    "    5: \"0GB\",\n",
    "    6: \"0GB\",\n",
    "    # 7: \"0GB\",\n",
    "}\n",
    "\n",
    "# max_memory_mapping = {\n",
    "#     0: \"0GB\",\n",
    "#     1: \"0GB\",\n",
    "#     2: \"24GB\",\n",
    "#     3: \"24GB\",\n",
    "# }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    use_auth_token=hf_token\n",
    "    # padding_side='left'\n",
    ")\n",
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory_mapping,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "lora_weights = \"YOUR_LORA_WEIGHTS\"\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    "    is_trainable=False,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory_mapping,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d158a314-94e3-4493-b7d5-c2903a609403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_OPENAI_KEY\"\n",
    "\n",
    "def generate_module_list_with_gpt(generated_module_seq):\n",
    "    todo_prompt = \"You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of 'module: module1, module: module2, module: module3, etc...'. \"\n",
    "    prompt = todo_prompt.format(generated_module_seq)\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    content = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    # print(content)\n",
    "    \n",
    "    content = content.split(\"module: \")[1:]\n",
    "    \n",
    "    result = \"\"\n",
    "    for c in content:\n",
    "        result += c\n",
    "    \n",
    "    # result = result[:-1] if len(result) > 0 else result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# generated_module_list = generate_module_list_with_gpt(response[prompt_length:])\n",
    "# print(generated_module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08c5c70-002a-4231-b54f-62d1c06f6540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nateraw/vit-base-beans were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at nateraw/vit-base-beans and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5737809c0f49fa8720ed82ad4cab8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading Evaluation Metrics\n",
    "\"\"\"\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt)\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt)\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# device_list = [\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:5\", \"cpu\"]\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c304bf7a-c38d-4c00-bbab-d0c378f0e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given low-resolutioned blurry grayscale image, how to return the regular image step by step?\n",
      "Image Denoising, Image Deblurring, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:24, 12.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:35, 11.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:47, 11.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:58, 11.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|███                                        | 1/14 [02:04<26:59, 124.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805744842529297\n",
      "Given blurry grayscale image, how to return the regular image step by step?\n",
      "Image Denoising, Colorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:18,  9.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:28,  9.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:37,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:46,  9.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|██████▏                                    | 2/14 [04:19<26:06, 130.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7587722473144531\n",
      "Given low-resolutioned blurry image, how to return the regular image step by step?\n",
      "Image Deblurring, Image Super Resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:17, 17.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:33, 16.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:49, 16.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [01:04, 15.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [01:21, 16.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|█████████▏                                 | 3/14 [06:59<26:26, 144.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6047112579345704\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the caption in German step by step?\n",
      "Image Denoising, Colorization, Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:17,  8.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:25,  8.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:33,  8.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:41,  8.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                              | 4/14 [09:45<25:26, 152.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46778397977352143\n",
      "Given low-resolutioned noisy blurry grayscale image, how to return the object names in English step by step?\n",
      "Image Denoising, Image Deblurring, Colorization, Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:18,  9.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:30, 10.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:41, 10.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:53, 10.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████▎                           | 5/14 [11:42<20:58, 139.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4233705446124077\n",
      "Given blurry grayscale image, how to return the object names in German step by step?\n",
      "Image Denoising, Colorization, Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:15,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:24,  8.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:33,  8.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:42,  8.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████▍                        | 6/14 [14:49<20:45, 155.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5442356157302857\n",
      "Given noisy grayscale image, how to return the caption in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████████████████████▌                     | 7/14 [16:56<17:05, 146.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Object Detection, Image Classification, Text to Image Generation, Machine Translation\n",
      "0\n",
      "Given low-resolutioned grayscale image, how to return the class label in English step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|████████████████████████▌                  | 8/14 [18:29<12:57, 129.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorization, Object Detection, Image Classification\n",
      "0\n",
      "Given low-resolutioned noisy blurry image, how to return the object names in German step by step?\n",
      "Image Denoising, Image Deblurring, Colorization, Object Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "\n",
      "1it [00:10, 10.07s/it]\u001b[A\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "\n",
      "2it [00:20, 10.45s/it]\u001b[A\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "\n",
      "3it [00:31, 10.57s/it]\u001b[A\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "\n",
      "4it [00:40,  9.88s/it]\u001b[A\u001b[AWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "\n",
      "\n",
      "5it [00:51, 10.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|███████████████████████████▋               | 9/14 [21:05<11:28, 137.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24602972485125063\n",
      "Given noisy blurry image, how to return the class label in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|██████████████████████████████            | 10/14 [21:35<06:57, 104.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Image Deblurring, Colorization\n",
      "0\n",
      "Given noisy blurry image, how to return the class label in German step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|█████████████████████████████████         | 11/14 [23:40<05:32, 110.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Denoising, Colorization, Object Detection, Image Classification, Machine Translation\n",
      "0\n",
      "Given low-resolutioned noisy image, how to return the caption in English step by step?\n",
      "Image Denoising, Image Deblurring, Colorization, Object Detection, Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:22, 11.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:31, 10.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:41, 10.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:52, 10.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████████      | 12/14 [26:00<03:59, 119.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48116223186254503\n",
      "Given English text, how to generate a image step by step?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|███████████████████████████████████████   | 13/14 [27:28<01:50, 110.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Image Generation, Sentiment Analysis, Object Detection\n",
      "0\n",
      "Given clozed English text, how to return the summarization in German step by step?\n",
      "Machine Translation, Text Summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:20, 10.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:29,  9.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:41, 10.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:55, 11.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 14/14 [29:00<00:00, 124.30s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 1/1 [29:00<00:00, 1740.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1968381667137146\n",
      "[0.0, 0.23594202635437247, 0.7230761159261068, 0.31967271409349307]\n",
      "[[0.0], [0.23594202635437247], [0.7230761159261068], [0.31967271409349307]]\n",
      "[0.0, 0.23594202635437247, 0.7230761159261068, 0.31967271409349307]\n",
      "Finished testing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n",
    "\n",
    "module_length = 10\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "\n",
    "eval_device = \"cuda:4\"\n",
    "\n",
    "random_seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "total_avg_clips = []\n",
    "total_avg_berts = []\n",
    "total_avg_similarities = []\n",
    "total_avg_rewards = []\n",
    "\n",
    "for idx, seed in enumerate(tqdm(random_seeds)):\n",
    "    torch.manual_seed(seed)\n",
    "    rewards = []\n",
    "    clips = []\n",
    "    berts = []\n",
    "    similarities = []\n",
    "    for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "        # if i == 1:\n",
    "        #     break\n",
    "            \n",
    "        print(task_description)\n",
    "        task_rewards = []\n",
    "        with torch.no_grad():\n",
    "            input_s = [\"### Human: \"+task_description+\"\\n### Assistant: \"]\n",
    "            input_ids = tokenizer.batch_encode_plus(\n",
    "                input_s, padding=\"longest\", return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].to(eval_device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=2048, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                num_beams=1,\n",
    "                output_hidden_states=True,\n",
    "                repetition_penalty=1.25\n",
    "            )\n",
    "    \n",
    "        generated_seq = tokenizer.decode(\n",
    "            output[\"sequences\"][0], skip_special_tokens=True, temperature=0, top_p=0.8, repetition_penalty=1.25\n",
    "        )\n",
    "\n",
    "        # print(generated_seq)\n",
    "\n",
    "        # generated_seq = generated_seq[len(input_s[0]):]\n",
    "        \n",
    "        # print(generated_seq)\n",
    "        \n",
    "        vicuna_steps = generate_module_list_with_gpt(generated_seq[len(input_s[0]):]).split(\",\")\n",
    "        module_list = match_module_seq(vicuna_steps, sentence_model)\n",
    "        # module_list = \"Image Denoising, Image Deblurring, Colorization\"\n",
    "        print(module_list)\n",
    "    \n",
    "        if len(module_list) >= 1 and whole_module_seq_filter(module_list, test_task_idx[i]):\n",
    "            seqCombination.construct_module_seq(module_list)\n",
    "    \n",
    "            for idx, batch in tqdm(enumerate(test_dataloaders[i])):\n",
    "                inputs = list(batch['input'][0])\n",
    "                # print(\"Inputs: \", inputs)\n",
    "                predictions = seqCombination.run_module_seq(inputs)\n",
    "                # try:\n",
    "                #     predictions = seqCombination.run_module_seq(inputs)\n",
    "                #     print(prediction)\n",
    "                # except:\n",
    "                #     ave_task_reward = 0\n",
    "                #     break\n",
    "    \n",
    "                if 0 <= test_task_idx[i] <= 14:\n",
    "                    outputs = list(batch['output'][0])\n",
    "                    dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                    task_rewards.append(dist / 100)\n",
    "                elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "                    outputs = list(batch['output'][0])\n",
    "                    f1 = np.mean(txt_eval(predictions, outputs, bertscore, device=eval_device))\n",
    "                    \n",
    "                    task_rewards.append(f1)\n",
    "                else:\n",
    "                    score = clip_score(predictions, inputs)\n",
    "                    task_rewards.append(score.detach()/100)\n",
    "                    \n",
    "            ave_task_reward = np.mean(task_rewards)    \n",
    "            seqCombination.close_module_seq()\n",
    "                \n",
    "        else:\n",
    "            ave_task_reward = 0\n",
    "    \n",
    "        print(ave_task_reward)\n",
    "            \n",
    "        if 0 <= test_task_idx[i] <= 14:\n",
    "            similarities.append(ave_task_reward)\n",
    "        elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "            berts.append(ave_task_reward)\n",
    "        else:\n",
    "            clips.append(ave_task_reward)\n",
    "    \n",
    "        rewards.append(ave_task_reward)     \n",
    "\n",
    "    avg_clips = np.mean(clips)\n",
    "    avg_berts = np.mean(berts)\n",
    "    avg_similarities = np.mean(similarities)\n",
    "    avg_rewards = (avg_clips + avg_berts + avg_similarities) / 3\n",
    "\n",
    "    print([avg_clips, avg_berts, avg_similarities, avg_rewards])\n",
    "\n",
    "    total_avg_clips.append(avg_clips)\n",
    "    total_avg_berts.append(avg_berts)\n",
    "    total_avg_similarities.append(avg_similarities)\n",
    "    total_avg_rewards.append(avg_rewards)\n",
    "    # print([avg_clips, avg_berts, avg_similarities, avg_rewards])\n",
    "\n",
    "print([total_avg_clips, total_avg_berts, total_avg_similarities, total_avg_rewards])\n",
    "\n",
    "print([np.mean(total_avg_clips), np.mean(total_avg_berts), np.mean(total_avg_similarities), np.mean(total_avg_rewards)])\n",
    "\n",
    "print(\"Finished testing!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf7b922-8eb8-47dd-9042-8c0b55f2dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12208757 0.24014789 0.75845707 0.37356417]\n"
     ]
    }
   ],
   "source": [
    "s1 = np.array([0.0, 0.18858468844741583, 0.801707077026367, 0.3300972551579276])\n",
    "s2 = np.array([0.0, 0.2142740244194865, 0.7530156758626302, 0.3224299000940389])\n",
    "s3 = np.array([0.3124078, 0.31503928893804545, 0.8067062479654948, 0.47805110950602425])\n",
    "s4 = np.array([0.29803005, 0.24689940550178288, 0.7077802251180013, 0.417569893076188])\n",
    "s5 = np.array([0.0, 0.23594202635437247, 0.7230761159261068, 0.31967271409349307])\n",
    "\n",
    "avg_s = (s1 + s2 + s3 + s4 + s5) / 5\n",
    "\n",
    "print(avg_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
