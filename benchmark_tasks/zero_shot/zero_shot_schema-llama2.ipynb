{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb19ce-a28d-44cc-b982-417d3d0366c5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoFeatureExtractor\n",
    "from general_dataset import GeneralDataset\n",
    "from agi_utils import *\n",
    "from tqdm import tqdm\n",
    "from undecorated import undecorated\n",
    "from types import MethodType\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "import random\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from combine_model_seq import SeqCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1e17d-2c29-4671-838a-d2f5185b1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "assign openagi data path \n",
    "\"\"\"\n",
    "data_path = \"YOUR_DATA_PATH\"\n",
    "\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "test_dataloaders = []\n",
    "for i in tqdm(test_task_idx):\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=20)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039c632-0136-4490-87ab-3fa1564e07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"eachadea/vicuna-7b-1.1\"\n",
    "base_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# base_model = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# base_model = \"chainyo/alpaca-lora-7b\"\n",
    "load_8bit = True\n",
    "\n",
    "hf_token = \"YOUR_HUGGINGFACE_KEY\"\n",
    "\n",
    "max_memory_mapping = {\n",
    "    0: \"48GB\",\n",
    "    1: \"48GB\",\n",
    "    2: \"48GB\",\n",
    "    3: \"48GB\",\n",
    "    4: \"0GB\",\n",
    "    5: \"0GB\",\n",
    "    6: \"0GB\",\n",
    "    # 7: \"0GB\",\n",
    "}\n",
    "\n",
    "# max_memory_mapping = {\n",
    "#     0: \"0GB\",\n",
    "#     1: \"0GB\",\n",
    "#     2: \"24GB\",\n",
    "#     3: \"24GB\",\n",
    "# }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    use_auth_token=hf_token\n",
    "    # padding_side='left'\n",
    ")\n",
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory_mapping,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "lora_weights = \"YOUR_LORA_WEIGHTS\"\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    "    is_trainable=False,\n",
    "    device_map=\"auto\",\n",
    "    max_memory=max_memory_mapping,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158a314-94e3-4493-b7d5-c2903a609403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_OPENAI_KEY\"\n",
    "\n",
    "def generate_module_list_with_gpt(generated_module_seq):\n",
    "    todo_prompt = \"You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of 'module: module1, module: module2, module: module3, etc...'. \"\n",
    "    prompt = todo_prompt.format(generated_module_seq)\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo-0613\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    content = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    # print(content)\n",
    "    \n",
    "    content = content.split(\"module: \")[1:]\n",
    "    \n",
    "    result = \"\"\n",
    "    for c in content:\n",
    "        result += c\n",
    "    \n",
    "    # result = result[:-1] if len(result) > 0 else result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# generated_module_list = generate_module_list_with_gpt(response[prompt_length:])\n",
    "# print(generated_module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c5c70-002a-4231-b54f-62d1c06f6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading Evaluation Metrics\n",
    "\"\"\"\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt)\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt)\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# device_list = [\"cuda:1\",\"cuda:2\",\"caugment_prompt:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:0\", \"cpu\"]\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304bf7a-c38d-4c00-bbab-d0c378f0e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n",
    "\n",
    "module_length = 10\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "\n",
    "eval_device = \"cuda:5\"\n",
    "\n",
    "random_seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "total_avg_clips = []\n",
    "total_avg_berts = []\n",
    "total_avg_similarities = []\n",
    "total_avg_rewards = []\n",
    "\n",
    "for idx, seed in enumerate(tqdm(random_seeds)):\n",
    "    torch.manual_seed(seed)\n",
    "    rewards = []\n",
    "    clips = []\n",
    "    berts = []\n",
    "    similarities = []\n",
    "    for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "        # if i == 1:\n",
    "        #     break\n",
    "            \n",
    "        print(task_description)\n",
    "        task_rewards = []\n",
    "        with torch.no_grad():\n",
    "            input_s = [\"### Human: \"+task_description+\"\\n### Assistant: \"]\n",
    "            # input_s = [context + \"Problem: \" + task_description + \"Solution:\\n\"]\n",
    "            input_ids = tokenizer.batch_encode_plus(\n",
    "                input_s, padding=\"longest\", return_tensors=\"pt\"\n",
    "            )[\"input_ids\"].to(eval_device)\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=2048, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                num_beams=1,\n",
    "                output_hidden_states=True,\n",
    "                repetition_penalty=1.25\n",
    "            )\n",
    "    \n",
    "        generated_seq = tokenizer.decode(\n",
    "            output[\"sequences\"][0], skip_special_tokens=True, temperature=0, top_p=0.8, repetition_penalty=1.25\n",
    "        )\n",
    "\n",
    "        # print(generated_seq)\n",
    "\n",
    "        # generated_seq = generated_seq[len(input_s[0]):]\n",
    "        \n",
    "        # print(generated_seq)\n",
    "        \n",
    "        vicuna_steps = generate_module_list_with_gpt(generated_seq[len(input_s[0]):]).split(\",\")\n",
    "        module_list = match_module_seq(vicuna_steps, sentence_model)\n",
    "        # module_list = \"Image Denoising, Image Deblurring, Colorization\"\n",
    "        print(module_list)\n",
    "    \n",
    "        if len(module_list) >= 1 and whole_module_seq_filter(module_list, test_task_idx[i]):\n",
    "            seqCombination.construct_module_seq(module_list)\n",
    "    \n",
    "            for idx, batch in tqdm(enumerate(test_dataloaders[i])):\n",
    "                inputs = list(batch['input'][0])\n",
    "                # print(\"Inputs: \", inputs)\n",
    "                predictions = seqCombination.run_module_seq(inputs)\n",
    "                # try:\n",
    "                #     predictions = seqCombination.run_module_seq(inputs)\n",
    "                #     print(prediction)\n",
    "                # except:\n",
    "                #     ave_task_reward = 0\n",
    "                #     break\n",
    "    \n",
    "                if 0 <= test_task_idx[i] <= 14:\n",
    "                    outputs = list(batch['output'][0])\n",
    "                    dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                    task_rewards.append(dist / 100)\n",
    "                elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "                    outputs = list(batch['output'][0])\n",
    "                    f1 = np.mean(txt_eval(predictions, outputs, bertscore, device=eval_device))\n",
    "                    \n",
    "                    task_rewards.append(f1)\n",
    "                else:\n",
    "                    score = clip_score(predictions, inputs)\n",
    "                    task_rewards.append(score.detach()/100)\n",
    "                    \n",
    "            ave_task_reward = np.mean(task_rewards)    \n",
    "            seqCombination.close_module_seq()\n",
    "                \n",
    "        else:\n",
    "            ave_task_reward = 0\n",
    "    \n",
    "        print(ave_task_reward)\n",
    "            \n",
    "        if 0 <= test_task_idx[i] <= 14:\n",
    "            similarities.append(ave_task_reward)\n",
    "        elif 15 <= test_task_idx[i] <= 104 or 107 <= test_task_idx[i]:\n",
    "            berts.append(ave_task_reward)\n",
    "        else:\n",
    "            clips.append(ave_task_reward)\n",
    "    \n",
    "        rewards.append(ave_task_reward)     \n",
    "\n",
    "    # print(\"clips\")\n",
    "    # print(clips)\n",
    "    # print(\"berts\")\n",
    "    # print(berts)\n",
    "    # print(\"similarities\")\n",
    "    # print(similarities)\n",
    "\n",
    "    avg_clips = np.mean(clips)\n",
    "    avg_berts = np.mean(berts)\n",
    "    avg_similarities = np.mean(similarities)\n",
    "    avg_rewards = (avg_clips + avg_berts + avg_similarities) / 3\n",
    "\n",
    "    res = [avg_clips, avg_berts, avg_similarities, avg_rewards]\n",
    "\n",
    "    print(res)\n",
    "\n",
    "    with open(\"zero-llama2.txt\", \"a\") as a:\n",
    "        write_str = \", \".join([str(i) for i in res]) + \"\\n\"\n",
    "        a.write(write_str)\n",
    "\n",
    "    total_avg_clips.append(avg_clips)\n",
    "    total_avg_berts.append(avg_berts)\n",
    "    total_avg_similarities.append(avg_similarities)\n",
    "    total_avg_rewards.append(avg_rewards)\n",
    "    # print([avg_clips, avg_berts, avg_similarities, avg_rewards])\n",
    "\n",
    "print([total_avg_clips, total_avg_berts, total_avg_similarities, total_avg_rewards])\n",
    "\n",
    "print([np.mean(total_avg_clips), np.mean(total_avg_berts), np.mean(total_avg_similarities), np.mean(total_avg_rewards)])\n",
    "\n",
    "print(\"Finished testing!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36123420-d640-4233-ab61-029d9827b86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_agi",
   "language": "python",
   "name": "peft_agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
